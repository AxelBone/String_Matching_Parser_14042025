{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567058da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import random\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # For older gensim versions\n",
    "import pyLDAvis.gensim_models  # For gensim 4+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d371c1d",
   "metadata": {},
   "source": [
    "## LDA sur rapports génétiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6060860f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6037, 4085)\n",
      "(5951, 4085)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"\"\n",
    "HPO_TERMS = \"../data/hpoterms08022021.txt\"\n",
    "\n",
    "mat = pd.read_csv(DATA_PATH, index_col=0)\n",
    "\n",
    "#### FILTER WITH SMALL TERMS\n",
    "mdMat = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0a246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_matrix_EHRxHPO_to_coherence(matrix):\n",
    "    \"\"\"\n",
    "    Convert an EHRxHPO binary matrix (DataFrame) into a list of lists for coherence analysis.\n",
    "    \n",
    "    Each row (EHR) is transformed into a list of present HPO terms.\n",
    "\n",
    "    Args:\n",
    "        matrix (pd.DataFrame): A binary matrix where rows are EHRs and columns are HPO terms (0/1).\n",
    "\n",
    "    Returns:\n",
    "        list of list of str: A list where each sublist contains active HPO terms for each EHR.\n",
    "    \"\"\"\n",
    "    # Convert each row into a list of HPO terms where the value is 1\n",
    "    topics = [list(matrix.columns[row == 1]) for _, row in matrix.iterrows()]\n",
    "    \n",
    "    return topics\n",
    "\n",
    "def removing_empty_ehr(topics):\n",
    "    new_topics = []\n",
    "\n",
    "    for topic in topics:\n",
    "        if topic != []:\n",
    "            new_topics.append(topic)\n",
    "    return new_topics\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f15f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = conversion_matrix_EHRxHPO_to_coherence(mdMat)\n",
    "\n",
    "texts = removing_empty_ehr(texts)\n",
    "\n",
    "# Create the dictionary from your tokenized documents\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "# Create the corpus (BoW representation)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb0a4a",
   "metadata": {},
   "source": [
    "## Recherche du nombre de topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67657b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_topics : 1\n",
      "num_topics : 2\n",
      "num_topics : 3\n",
      "num_topics : 4\n",
      "num_topics : 5\n",
      "num_topics : 6\n",
      "num_topics : 7\n",
      "num_topics : 8\n",
      "num_topics : 9\n",
      "num_topics : 10\n",
      "num_topics : 11\n",
      "num_topics : 12\n",
      "num_topics : 13\n",
      "num_topics : 14\n",
      "num_topics : 15\n",
      "num_topics : 16\n",
      "num_topics : 17\n",
      "num_topics : 18\n",
      "num_topics : 19\n",
      "num_topics : 20\n",
      "num_topics : 21\n",
      "num_topics : 22\n",
      "num_topics : 23\n",
      "num_topics : 24\n",
      "num_topics : 25\n",
      "num_topics : 26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnum_topics :\u001b[39m\u001b[33m\"\u001b[39m, num_topics)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Train LDA model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m lda_model = \u001b[43mLdaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Calculate Perplexity\u001b[39;00m\n\u001b[32m     14\u001b[39m perplexity = lda_model.log_perplexity(corpus)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cluster_analysis/lib/python3.12/site-packages/gensim/models/ldamodel.py:521\u001b[39m, in \u001b[36mLdaModel.__init__\u001b[39m\u001b[34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[39m\n\u001b[32m    519\u001b[39m use_numpy = \u001b[38;5;28mself\u001b[39m.dispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    520\u001b[39m start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_as_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_numpy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[38;5;28mself\u001b[39m.add_lifecycle_event(\n\u001b[32m    523\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcreated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    524\u001b[39m     msg=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    525\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cluster_analysis/lib/python3.12/site-packages/gensim/models/ldamodel.py:1006\u001b[39m, in \u001b[36mLdaModel.update\u001b[39m\u001b[34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1002\u001b[39m     logger.info(\n\u001b[32m   1003\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPROGRESS: pass \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m, at document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1004\u001b[39m         pass_, chunk_no * chunksize + \u001b[38;5;28mlen\u001b[39m(chunk), lencorpus\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     gammat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_estep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1008\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.optimize_alpha:\n\u001b[32m   1009\u001b[39m         \u001b[38;5;28mself\u001b[39m.update_alpha(gammat, rho())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cluster_analysis/lib/python3.12/site-packages/gensim/models/ldamodel.py:768\u001b[39m, in \u001b[36mLdaModel.do_estep\u001b[39m\u001b[34m(self, chunk, state)\u001b[39m\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    767\u001b[39m     state = \u001b[38;5;28mself\u001b[39m.state\n\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m gamma, sstats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollect_sstats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    769\u001b[39m state.sstats += sstats\n\u001b[32m    770\u001b[39m state.numdocs += gamma.shape[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# avoids calling len(chunk) on a generator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cluster_analysis/lib/python3.12/site-packages/gensim/models/ldamodel.py:701\u001b[39m, in \u001b[36mLdaModel.inference\u001b[39m\u001b[34m(self, chunk, collect_sstats)\u001b[39m\n\u001b[32m    699\u001b[39m     ids = [\u001b[38;5;28mint\u001b[39m(idx) \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     ids = [idx \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[32m    702\u001b[39m cts = np.fromiter((cnt \u001b[38;5;28;01mfor\u001b[39;00m _, cnt \u001b[38;5;129;01min\u001b[39;00m doc), dtype=\u001b[38;5;28mself\u001b[39m.dtype, count=\u001b[38;5;28mlen\u001b[39m(doc))\n\u001b[32m    703\u001b[39m gammad = gamma[d, :]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "coherence_scores = []\n",
    "\n",
    "start_k = 2\n",
    "n_topics = 200\n",
    "\n",
    "topic_range = range(start_k, n_topics + 1)\n",
    "\n",
    "# Test topic numbers from 2 to 200\n",
    "for num_topics in topic_range:\n",
    "    print(\"num_topics :\", num_topics)\n",
    "    # Train LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)\n",
    "    \n",
    "    # Calculate Perplexity\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    perplexities.append(perplexity)\n",
    "    \n",
    "    # Calculate Coherence\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8aeb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "# Plot Perplexity\n",
    "ax1.set_xlabel('Number of Topics')\n",
    "ax1.set_ylabel('Perplexity', color='tab:blue')\n",
    "ax1.plot(topic_range, perplexities, color=\"tab:blue\", label=\"Perplexity\")\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Create second y-axis to plot Coherence\n",
    "ax2 = ax1.twinx() \n",
    "ax2.set_ylabel('Coherence Score', color='tab:green')\n",
    "ax2.plot(topic_range, coherence_scores, label=\"Coherence\", color=\"tab:green\")\n",
    "ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "\n",
    "# Title and legend\n",
    "plt.suptitle('Perplexity and Coherence vs. Number of Topics \\n With filtering rare tokens')\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../output/coherence_perplexity_with_filtering_150_300groupes_2025_03_19.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1155059",
   "metadata": {},
   "source": [
    "## Observing LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d148d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding HPO terms dictionary\n",
    "def reading_hpo_terms(path: str) -> dict:\n",
    "    d = {}\n",
    "    with open(path, \"r\") as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()  # Remove newline & spaces\n",
    "            if not line:  \n",
    "                continue  # Skip empty lines\n",
    "            \n",
    "            # Split on last space/tab to get (name, ID)\n",
    "            try:\n",
    "                val, key = line.rsplit(maxsplit=1)  # Works for both space & tab-separated\n",
    "            except ValueError:\n",
    "                print(f\"Skipping invalid line: {line}\")  # Debugging bad format\n",
    "                continue\n",
    "            \n",
    "            # Keep only the first occurrence of the key\n",
    "            if key not in d:\n",
    "                d[key] = val \n",
    "\n",
    "    return d\n",
    "\n",
    "hpo_dict = reading_hpo_terms(HPO_TERMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff642fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert IDs to names\n",
    "texts_named = [[hpo_dict.get(term, term) for term in doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choix TOPICS\n",
    "num_topics = 1\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary, mds=\"mmds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
