{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612ac8a7",
   "metadata": {},
   "source": [
    "# Comparison of Gene Prioritization Methods / Runs\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook compares multiple gene prioritization runs (tools, HPO versions, extraction strategies)\n",
    "on the same cohort of patients with known causal genes.\n",
    "\n",
    "The goals are to:\n",
    "- assess global prioritization performance across runs\n",
    "- compare robustness and failure modes\n",
    "- evaluate the impact of phenotype quantity and quality\n",
    "- identify strengths and weaknesses of each method\n",
    "\n",
    "All analyses are performed at the **patient level**, using the rank of the known causal gene.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790c028",
   "metadata": {},
   "source": [
    "## Imports and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c7a57",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PARAMÈTRE ===\n",
    "ANALYSIS_TABLE_PATH = \"analysis_table.csv\"  # ou .parquet\n",
    "\n",
    "# Chargement\n",
    "if ANALYSIS_TABLE_PATH.endswith(\".parquet\"):\n",
    "    df = pd.read_parquet(ANALYSIS_TABLE_PATH)\n",
    "else:\n",
    "    df = pd.read_csv(ANALYSIS_TABLE_PATH)\n",
    "\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Runs:\", df[\"run_id\"].nunique())\n",
    "print(\"Patients:\", df[\"ID_PAT_ETUDE\"].nunique())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3cc22",
   "metadata": {},
   "source": [
    "## Verification of runs comparability\n",
    "\n",
    "ensure that the runs are based on a comparable basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9162fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_summary = (\n",
    "    df.groupby(\"run_id\")\n",
    "    .agg(\n",
    "        n_rows=(\"ID_PAT_ETUDE\", \"count\"),\n",
    "        n_patients=(\"ID_PAT_ETUDE\", \"nunique\"),\n",
    "        reports_found=(\"report_found\", \"sum\"),\n",
    "        read_errors=(\"report_read_error\", \"sum\"),\n",
    "        gene_not_found=(\"gene_not_found_flag\", \"sum\"),\n",
    "    )\n",
    "    .sort_values(\"n_patients\", ascending=False)\n",
    ")\n",
    "\n",
    "run_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e75972",
   "metadata": {},
   "source": [
    "## Filtering usable lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lignes exploitables pour l'évaluation du rang\n",
    "eval_df = df[\n",
    "    df[\"report_found\"] &\n",
    "    (~df[\"report_read_error\"])\n",
    "].copy()\n",
    "\n",
    "print(\"Rows usable for evaluation:\", len(eval_df))\n",
    "print(\"Patients usable:\", eval_df[\"ID_PAT_ETUDE\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8150a",
   "metadata": {},
   "source": [
    "## Construction of the median rank per patient and per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1810a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_eval = (\n",
    "    eval_df\n",
    "    .groupby([\"run_id\", \"ID_PAT_ETUDE\"])\n",
    "    .agg(\n",
    "        avg_rank=(\"rank\", \"mean\"),      # ou \"min\" si tu préfères\n",
    "        phenotype_length=(\"hpo_implicated\", lambda x: x.notna().sum())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "rank_eval.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbd93f",
   "metadata": {},
   "source": [
    "## Global performance: Top-N per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_at_k(series, k):\n",
    "    return (series.notna() & (series <= k)).mean() * 100\n",
    "\n",
    "topN_summary = (\n",
    "    rank_eval\n",
    "    .groupby(\"run_id\")[\"avg_rank\"]\n",
    "    .apply(lambda s: pd.Series({\n",
    "        \"Top1_%\": hit_at_k(s, 1),\n",
    "        \"Top5_%\": hit_at_k(s, 5),\n",
    "        \"Top10_%\": hit_at_k(s, 10),\n",
    "        \"Top20_%\": hit_at_k(s, 20),\n",
    "        \"Top50_%\": hit_at_k(s, 50),\n",
    "    }))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "topN_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad110b02",
   "metadata": {},
   "source": [
    "## Top-N comparative bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787445b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topN_long = topN_summary.melt(\n",
    "    id_vars=\"run_id\",\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"percentage\"\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    topN_long,\n",
    "    x=\"run_id\",\n",
    "    y=\"percentage\",\n",
    "    color=\"metric\",\n",
    "    barmode=\"group\",\n",
    "    title=\"Top-N performance comparison across runs\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"simple_white\",\n",
    "    xaxis_title=\"Run / Method\",\n",
    "    yaxis_title=\"Percentage of patients (%)\",\n",
    "    legend_title=\"Metric\",\n",
    "    yaxis=dict(range=[0, 100]),\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f64c4",
   "metadata": {},
   "source": [
    "## Per run ranks distribution (boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe64e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(\n",
    "    rank_eval,\n",
    "    x=\"run_id\",\n",
    "    y=\"avg_rank\",\n",
    "    points=\"all\",\n",
    "    title=\"Distribution of causal gene ranks across runs\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"simple_white\",\n",
    "    xaxis_title=\"Run / Method\",\n",
    "    yaxis_title=\"Average rank of the causal gene\",\n",
    "    yaxis=dict(autorange=\"reversed\"),\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038651f1",
   "metadata": {},
   "source": [
    "## Comparative CDF of ranks (Recall curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ad1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = 50\n",
    "cdf_data = []\n",
    "\n",
    "for run_id in rank_eval[\"run_id\"].unique():\n",
    "    sub = rank_eval[rank_eval[\"run_id\"] == run_id]\n",
    "    total = len(sub)\n",
    "    if total == 0:\n",
    "        continue\n",
    "    for k in range(1, k_max + 1):\n",
    "        pct = (sub[\"avg_rank\"].notna() & (sub[\"avg_rank\"] <= k)).sum() / total\n",
    "        cdf_data.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"k\": k,\n",
    "            \"percentage\": pct * 100\n",
    "        })\n",
    "\n",
    "cdf_df = pd.DataFrame(cdf_data)\n",
    "\n",
    "fig = px.line(\n",
    "    cdf_df,\n",
    "    x=\"k\",\n",
    "    y=\"percentage\",\n",
    "    color=\"run_id\",\n",
    "    title=\"CDF of causal gene ranks across runs\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"simple_white\",\n",
    "    xaxis_title=\"Rank k\",\n",
    "    yaxis_title=\"Percentage of patients (%)\",\n",
    "    xaxis=dict(tickvals=[1,10,20,30,40,50]),\n",
    "    yaxis=dict(range=[0, 100]),\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fa908",
   "metadata": {},
   "source": [
    "## To add:\n",
    "\n",
    "- Comparison at equal phenotype size (handling phenotypic size depends of HPO version too)\n",
    "- Effect of phenotype specificity across runs\n",
    "- Failure mode analysis per run\n",
    "- ClinVar / ACMG stratified comparison\n",
    "- Gene-centric difficulty across runs\n",
    "- Inter-run concordance & stability\n",
    "- Multivariate comparative model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
